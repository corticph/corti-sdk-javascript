/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Corti from "../../../../index.js";

/**
 * @example
 *     {
 *         recordingId: "f47ac10b-58cc-4372-a567-0e02b2c3d479",
 *         primaryLanguage: "en",
 *         modelName: "base"
 *     }
 */
export interface TranscriptsCreateRequest {
    /** The unique identifier for the recording. */
    recordingId: Corti.Uuid;
    /** The primary spoken language of the recording. Check https://docs.corti.ai/about/languages for more. */
    primaryLanguage: Corti.TranscriptsCreateRequestPrimaryLanguage;
    /** Indicates whether spoken dictation commands should be converted to punctuation (e.g., 'comma' â†’ ','). */
    isDictation?: boolean;
    /** If true, each audio channel is transcribed separately. */
    isMultichannel?: boolean;
    /** If true, separates speakers within an audio channel returning incrementing ids for transcript segments. */
    diarize?: boolean;
    /** An array of participants, each specifying a role and an assigned audio channel in the recording. Leave empty when shouldDiarize: true */
    participants?: Corti.TranscriptsParticipant[];
    /** The model name for transcription. By default, only the highest tier is accessible. Check https://docs.corti.ai/about/languages for more. */
    modelName: Corti.TranscriptsCreateRequestModelName;
}
